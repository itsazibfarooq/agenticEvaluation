# **Evaluating and regulating agentic AI: a study of benchmarks, metrics, and regulation**

Our work builds on a wide array of research in Agentic AI, governance, and evaluation. The following is the complete, categorized bibliography from our paper.



### Agentic AI & MAS (Surveys & General Concepts)

- Acharya, D. B., et al. (2025). _Agentic ai: Autonomous intelligence for complex goals--a comprehensive survey_. IEEE Access.
- Bandi, A., et al. (2025). _The Rise of Agentic AI: A Review of Definitions, Frameworks, Architectures, Applications, Evaluation Metrics, and Challenges_. Future Internet.
- Chowa, S. S., et al. (2025). _From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users_. arXiv.
- Guo, T., et al. (2024). _Large language model based multi-agents: A survey of progress and challenges_. arXiv.
- Hughes, L., et al. (2025). _AI agents and agentic systems: A multi-expert analysis_. Journal of Computer Information Systems.
- Nisa, U., et al. (2025). _Agentic AI: The age of reasoning—A review_. Journal of Automation and Intelligence.
- Piccialli, F., et al. (2025). _AgentAI: A Comprehensive Survey on Autonomous Agents in Distributed AI for Industry 4.0_. Expert Systems with Applications.
- Plaat, A., et al. (2025). _Agentic Large Language Models, a survey_. arXiv.
- Qu, X., et al. (2025). _A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond_. arXiv.
- Russell, S. J. & Norvig, P. (2021). _Artificial Intelligence: A Modern Approach_ (4th ed.). Pearson.
- Wang, L., et al. (2024). _A survey on large language model based autonomous agents_. Frontiers of Computer Science.
- Wooldridge, M. (2009). _An Introduction to MultiAgent Systems_ (2nd ed.). John Wiley & Sons.

### Agent Architectures, Reasoning & Frameworks

- Ahn, M., et al. (2022). _Do as i can, not as i say: Grounding language in robotic affordances_. arXiv.
- Belcak, P., et al. (2025). _Small Language Models are the Future of Agentic AI_. arXiv.
- Bran, A. M., et al. (2023). _Chemcrow: Augmenting large-language models with chemistry tools_. arXiv.
- Chen, G., et al. (2023). _Autoagents: A framework for automatic agent generation_. arXiv.
- Chen, W., et al. (2023). _Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks_. arXiv.
- Derouiche, H., et al. (2025). _Agentic AI Frameworks: Architectures, Protocols, and Design Challenges_. arXiv.
- Feng, K. J., et al. (2025). _Levels of Autonomy for AI Agents_. Knight First Amendment Institute at Columbia University.
- Haase, J. & Pokutta, S. (2025). _Beyond Static Responses: Multi-Agent LLM Systems as a New Paradigm for Social Science Research_. arXiv.
- Hong, S., et al. (2024). _MetaGPT: Meta programming for a multi-agent collaborative framework_. ICLR.
- Lewis, P., et al. (2020). _Retrieval-augmented generation for knowledge-intensive nlp tasks_. NeurIPS.
- Li, D., et al. (2025). _SMoA: Improving Multi-agent Large Language Models with S parse M ixture-o f-A gents_. PAKDD.
- Li, G., et al. (2023). _Camel: Communicative agents for" mind" exploration of large language model society_. NeurIPS.
- Nakajima, Y. (2023). _BabyAGI: An Autonomous Task Management System_. GitHub.
- Ouyang, L., et al. (2022). _Training language models to follow instructions with human feedback_. NeurIPS.
- Park, J. S., et al. (2023). _Generative Agents: Interactive Simulacra of Human Behavior_. UIST.
- Schick, T., et al. (2023). _Toolformer: Language Models Can Teach Themselves to Use Tools_. arXiv.
- Shinn, N., et al. (2023). _Reflexion: Language agents with verbal reinforcement learning_. NeurIPS.
- Talebirad, Y. & Nadiri, A. (2023). _Multi-agent collaboration: Harnessing the power of intelligent llm agents_. arXiv.
- Wang, F., et al. (2025). _Rag+: Enhancing retrieval-augmented generation with application-aware reasoning_. arXiv.
- Wang, G., et al. (2023). _Voyager: An open-ended embodied agent with large language models_. arXiv.
- Wang, J., et al. (2024). _Mixture-of-agents enhances large language model capabilities_. arXiv.
- Wang, X., et al. (2022). _Self-consistency improves chain of thought reasoning in language models_. arXiv.
- Wu, X., et al. (2024). _Can graph learning improve planning in LLM-based agents?_. NeurIPS.
- Yang, H., et al. (2023). _Auto-gpt for online decision making: Benchmarks and additional opinions_. arXiv.
- Yao, S., et al. (2023). _React: Synergizing reasoning and acting in language models_. ICLR.

### Governance, Risk, Security & Safety

- Ahmed, S. Q. (2025). _Agentic AI: A Governance Wake-Up Call_. NACD Directorship Magazine.
- Andriushchenko, M., et al. (2024). _Agentharm: A benchmark for measuring harmfulness of llm agents_. arXiv.
- Anthropic. (2025). _Responsible Scaling Policy, Version 2.1_.
- Debenedetti, E., et al. (2024). _Agentdojo: A dynamic environment to evaluate prompt injection attacks and defenses for llm agents_. NeurIPS.
- Dong, Y., et al. (2024). _Building guardrails for large language models_. arXiv.
- European Union. (2024). _Regulation (EU) 2024/1689... (Artificial Intelligence Act)_. Official Journal of the European Union.
- ISO & IEC. (2023). _ISO/IEC 42001:2023 --- Artificial intelligence — Management system_.
- Levy, I., et al. (2024). _St-webagentbench: A benchmark for evaluating safety and trustworthiness in web agents_. arXiv.
- Lu, X., et al. (2025). _IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks_. arXiv.
- Marks, S., et al. (2025). _Auditing Language Models for Hidden Objectives_. arXiv.
- Marks, S., et al. (2025). _Building and evaluating alignment auditing agents_. AI Alignment Forum.
- MITRE Corporation. (2025). _ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems_.
- NIST. (2020). _Zero Trust Architecture_ (SP 800-207).
- NIST. (2023). _Artificial Intelligence Risk Management Framework (AI RMF 1.0)_ (NIST AI 100-1).
- NTIA. (2021). _The Minimum Elements for a Software Bill of Materials (SBOM)_.
- Open Policy Agent Project. (2024). _Open Policy Agent: Policy as Code_.
- OWASP Foundation. (2025). _OWASP Top 10 for Large Language Model Applications (2025)_.
- Raza, S., et al. (2025). _Trism for agentic ai: A review of trust, risk, and security management..._. arXiv.
- Ruan, Y., et al. (2023). _Identifying the risks of lm agents with an lm-emulated sandbox_. arXiv.
- Sherman, E., et al. (2025). _From Assistant to Agent: Navigating the Governance Challenges of Increasingly Autonomous AI_. Credo AI.
- Shukla, M. (2025). _Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems_. arXiv.
- SLSA Community. (2023). _SLSA Specification v1.0_.
- Thurgood, S., et al. (2018). _Example Error Budget Policy_. Google SRE.
- Torres-Arias, S., et al. (2019). _in-toto: Providing Farm-to-Table Guarantees for Bits and Bytes_. USENIX Security.
- Verifiable Credentials Working Group. (2025). _Verifiable Credentials Data Model 2.0_. W3C.
- Wang, N., et al. (2025). _Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation_. arXiv.
- Yin, S., et al. (2024). _Safeagentbench: A benchmark for safe task planning of embodied llm agents_. arXiv.
- Yu, M., et al. (2025). _A survey on trustworthy llm agents: Threats and countermeasures_. KDD.

### Responsible AI, Bias & Human-in-the-Loop

- Mosqueira-Rey, E., et al. (2023). _Human-in-the-loop machine learning: a state of the art_. Artificial Intelligence Review.
- Raza, S., et al. (2024). _Exploring bias and prediction metrics to characterise the fairness of machine learning..._. IEEE Access.
- Raza, S., et al. (2025). _Developing safe and responsible large language model: can we balance bias reduction and language understanding?_. Machine Learning.
- Raza, S., et al. (2025). _Humanibench: A human-centric framework for large multimodal models evaluation_. arXiv.
- Raza, S., et al. (2025). _Responsible Agentic Reasoning and AI Agents: A Critical Survey_. Authorea Preprints.
- Raza, S., et al. (2025). _ViLBias: Detecting and Reasoning about Bias in Multimodal Content_. arXiv.
- Raza, S., et al. (2025). _Who is responsible? the data, models, users or regulations? a comprehensive survey..._. arXiv.

### Agent Memory

- Cherepanov, E., et al. (2025). _Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks..._. arXiv.
- Pasukonis, J., et al. (2022). _Evaluating long-term memory in 3d mazes_. arXiv.
- Wang, F., et al. (2025). _Text2Mem: A Unified Memory Operation Language for Memory Operating System_. arXiv.
- Xia, M., et al. (2025). _Minerva: A Programmable Memory Test Benchmark for Language Models_. arXiv.
- Xu, W., et al. (2025). _A-mem: Agentic memory for llm agents_. arXiv.

### Benchmarking: General & Multi-Domain

- Chang, M., et al. (2024). _Agentboard: An analytical evaluation board of multi-turn llm agents_. NeurIPS.
- Gioacchini, L., et al. (2024). _Agentquest: A modular benchmark framework to measure progress and improve llm agents_. arXiv.
- IBM. (2025). _Agentic AI evaluation_. IBM Docs.
- Li, M., et al. (2023). _Api-bank: A comprehensive benchmark for tool-augmented llms_. arXiv.
- Liu, X., et al. (2023). _AgentBench: Evaluating LLMs as Agents_. arXiv.
- Mialon, G., et al. (2023). _Gaia: a benchmark for general ai assistants_. ICLR.
- Mohammadi, M., et al. (2025). _Evaluation and benchmarking of llm agents: A survey_. KDD.
- Nath, V., et al. (2025). _Toolcomp: A multi-tool reasoning & process supervision benchmark_. arXiv.
- Patil, S. G., et al. (2024). _Gorilla: Large language model connected with massive apis_. NeurIPS.
- Qin, Y., et al. (2023). _Toolllm: Facilitating large language models to master 16000+ real-world apis_. arXiv.
- Shen, Y., et al. (2024). _Taskbench: Benchmarking large language models for task automation_. NeurIPS.
- Uchendu, I., et al. (2025). _A2Perf: Real-World Autonomous Agents Benchmark_. arXiv.
- Wang, X., et al. (2023). _Mint: Evaluating llms in multi-turn interaction with tools and language feedback_. arXiv.
- Wu, C. K., et al. (2024). _Streambench: Towards benchmarking continuous improvement of language agents_. NeurIPS.
- Xu, F. F., et al. (2024). _Theagentcompany: benchmarking llm agents on consequential real world tasks_. arXiv.
- Xu, Q., et al. (2023). _On the tool manipulation capability of open-source large language models_. arXiv.
- Yao, S., et al. (2024). _tau-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains_. arXiv.
- Yehudai, A., et al. (2025). _Survey on evaluation of llm-based agents_. arXiv.

### Benchmarking: Web, Desktop & UI Agents

- Chen, J., et al. (2024). _Spa-bench: A comprehensive benchmark for smartphone agent evaluation_. NeurIPS Workshop.
- Chezelles, D., et al. (2024). _The browsergym ecosystem for web agent research_. arXiv.
- Deng, X., et al. (2023). _Mind2web: Towards a generalist agent for the web_. NeurIPS.
- Gou, B., et al. (2025). _Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge_. arXiv.
- Kapoor, R., et al. (2024). _Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents..._. ECCV.
- Koh, J. Y., et al. (2024). _Visualwebarena: Evaluating multimodal agents on realistic visual web tasks_. arXiv.
- Pan, Y., et al. (2024). _Webcanvas: Benchmarking web agents in online environments_. arXiv.
- Peeters, R., et al. (2025). _WebMall--A Multi-Shop Benchmark for Evaluating Web Agents_. arXiv.
- Tian, S., et al. (2024). _Mmina: Benchmarking multihop multimodal internet agents_. arXiv.
- Trivedi, H., et al. (2024). _Appworld: A controllable world of apps and people for benchmarking interactive coding agents_. arXiv.
- Wang, H., et al. (2025). _Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning_. arXiv.
- Wang, J., et al. (2024). _Hammerbench: Fine-grained function-calling evaluation in real mobile device scenarios_. arXiv.
- Xie, T., et al. (2024). _Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments_. NeurIPS.
- Xing, M., et al. (2024). _Understanding the weakness of large language model agents within a complex android environment_. KDD.
- Yao, S., et al. (2022). _Webshop: Towards scalable real-world web interaction with grounded language agents_. NeurIPS.
- Yoran, O., et al. (2024). _Assistantbench: Can web agents solve realistic and time-consuming tasks?_. arXiv.
- Zhou, S., et al. (2023). _Webarena: A realistic web environment for building autonomous agents_. arXiv.

### Benchmarking: Software, ML & Data Science

- Bogin, B., et al. (2024). _Super: Evaluating agents on setting up and executing tasks from research repositories_. arXiv.
- Chan, J. S., et al. (2024). _Mle-bench: Evaluating machine learning agents on machine learning engineering_. arXiv.
- Deshpande, D., et al. (2025). _TRAIL: Trace Reasoning and Agentic Issue Localization_. arXiv.
- Huang, B., et al. (2025). _DCA-Bench: A Benchmark for Dataset Curation Agents_. KDD.
- Huang, Q., et al. (2023). _Mlagentbench: Evaluating language agents on machine learning experimentation_. arXiv.
- Jha, S., et al. (2025). _Itbench: Evaluating ai agents across diverse real-world it automation tasks_. arXiv.
- Jimenez, C. E., et al. (2023). _Swe-bench: Can language models resolve real-world github issues?_. arXiv.
- Li, K., et al. (2025). _Datasetresearch: Benchmarking agent systems for demand-driven dataset discovery_. arXiv.
- Miserendino, S., et al. (2025). _SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?_. arXiv.
- Padigela, H., et al. (2025). _Ml-dev-bench: Comparative analysis of ai agents on ml development workflows_. arXiv.
- Rashid, M. S., et al. (2025). _SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents_. arXiv.
- Rein, D., et al. (2025). _HCAST: Human-Calibrated Autonomy Software Tasks_. arXiv.
- Siegel, Z. S., et al. (2024). _Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark_. arXiv.
- Starace, G., et al. (2025). _PaperBench: Evaluating AI's Ability to Replicate AI Research_. arXiv.
- Wijk, H., et al. (2024). _Re-bench: Evaluating frontier ai r\&d capabilities of language model agents..._. arXiv.

### Benchmarking: Multi-Agent & Collaboration

- Agashe, S., et al. (2023). _Llm-coordination: evaluating and analyzing multi-agent coordination abilities..._. arXiv.
- Chen, J., et al. (2024). _Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments_. arXiv.
- Chen, H., et al. (2024). _Socialbench: Sociality evaluation of role-playing conversational agents_. arXiv.
- Dong, Y., et al. (2024). _Villageragent: A graph-based multi-agent framework for coordinating complex task dependencies..._. arXiv.
- Gong, R., et al. (2023). _Mindagent: Emergent gaming interaction_. arXiv.
- Hyun, J., et al. (2025). _CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale_. arXiv.
- Mandi, Z., et al. (2024). _Roco: Dialectic multi-robot collaboration with large language models_. ICRA.
- Qi, S., et al. (2024). _Civrealm: A learning and reasoning odyssey in civilization for decision-making agents_. arXiv.
- Sun, H., et al. (2025). _Collab-Overcooked: Benchmarking and evaluating large language models as collaborative agents_. arXiv.
- Wang, W., et al. (2024). _Battleagentbench: A benchmark for evaluating cooperation and competition capabilities..._. arXiv.
- Zhou, X., et al. (2L: 023). _Sotopia: Interactive evaluation for social intelligence in language agents_. arXiv.
- Zhou, Y., et al. (2025). _Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks_. arXiv.
- Zhu, K., et al. (2025). _Multiagentbench: Evaluating the collaboration and competition of llm agents_. arXiv.

### Benchmarking: Planning & Reasoning

- Bogavelli, T., et al. (2025). _AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise_. arXiv.
- Chen, L., et al. (2024). _Mindbench: A comprehensive benchmark for mind map structure recognition and analysis_. arXiv.
- Geng, L. & Chang, E. Y. (2025). _Realm-bench: A real-world planning benchmark for llms and multi-agent systems_. arXiv.
- Kokel, H., et al. (2025). _Acpbench: Reasoning about action, change, and planning_. AAAI.
- Li, L., et al. (2024). _Reflection-Bench: Evaluating Epistemic Agency in Large Language Models_. arXiv.
- Liu, Y., et al. (2024). _Tool-planner: Task planning with clusters across multiple tools_. arXiv.
- Stein, K., et al. (2023). _Autoplanbench: Automatically generating benchmarks for llm planners from pddl_. arXiv.
- Valmeekam, K., et al. (2023). _Planbench: An extensible benchmark for evaluating large language models on planning..._. NeurIPS.
- Xiao, R., et al. (2024). _Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents_. arXiv.
- Zhang, Y., et al. (2024). _Timearena: Shaping efficient multitasking language agents in a time-aware simulation_. arXiv.
- Zheng, H. S., et al. (2024). _Natural plan: Benchmarking llms on natural language planning_. arXiv.

### Benchmarking: Embodied, Vision & Multimodal

- Huang, J., et al. (2024). _Mmevalpro: Calibrating multimodal benchmarks towards trustworthy and efficient evaluation_. arXiv.
- Li, M., et al. (2024). _Embodied agent interface: Benchmarking llms for embodied decision making_. NeurIPS.
- Ma, Z., et al. (2024). _m & m’s: A benchmark to evaluate tool-use for m ulti-step m ulti-modal tasks_. ECCV.
- Shridhar, M., et al. (2020). _Alfworld: Aligning text and embodied environments for interactive learning_. arXiv.
- Yang, R., et al. (2025). _Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents_. arXiv.

### Benchmarking: Specialized Domains

- Achim, T., et al. (2025). _Aristotle: IMO-level Automated Theorem Proving_. arXiv.
- Chen, L., et al. (2025). _Seed-prover: Deep and broad reasoning for automated theorem proving_. arXiv.
- Chen, Z., et al. (2024). _Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery_. arXiv.
- Moteki, A., et al. (2025). _FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks_. arXiv.
- Zhang, X., et al. (2025). _Eduplanner: Llm-based multi-agent systems for customized and intelligent instructional design_. IEEE Transactions on Learning Technologies.

### Core Technologies & Foundational Datasets

- Artacho, B. & Savakis, A. (2021). _Omnipose: A multi-scale framework for multi-person pose estimation_. arXiv.
- Bordes, F., et al. (2024). _An introduction to vision-language modeling_. arXiv.
- Huang, C., et al. (2025). _R-Zero: Self-Evolving Reasoning LLM from Zero Data_. arXiv.
- Kumar, S., et al. (2024). _The need for a big world simulator: A scientific challenge for continual learning_. arXiv.
- Lin, J., et al. (2024). _Ct-glip: 3d grounded language-image pretraining with ct scans and radiology reports..._. arXiv.
- Yang, Z., et al. (2018). _HotpotQA: A dataset for diverse, explainable multi-hop question answering_. arXiv.
- Ye, D., et al. (2025). _Yan: Foundational Interactive Video Generation_. arXiv.
- Zhang, T., et al. (2021). _C-planning: An automatic curriculum for learning goal-reaching tasks_. arXiv.
- Zhang, Z., et al. (2025). _RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics_. arXiv.


---
```
@article{farooq2025agentic,
  title={Evaluating and Regulating Agentic AI:  A Study of Benchmarks, Metrics, and Regulation},
  author={Farooq, Azib; Raza, Shaina, Karim, Nazmul; Iqbal, Hassan; Emmanouilidis, Christos},
  journal={},
  year={2025}
}
```

